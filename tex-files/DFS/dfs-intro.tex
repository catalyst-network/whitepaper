Catalyst integrates its own Distributed File System (DFS) \cite{DFS} based on the InterPlanetary File System (IPFS) protocol \cite{benet2014ipfs}. DFS, as the name suggests, is a peer to peer file system, where the files contained within are distributed across and accessible to a range of peers on a network. This file system will have no down time nor any single point of failure (baring complete downtime of the internet), meaning that it is persistent in a way that other, centralised file storage technologies are not. IPFS is a peer to peer protocol that allows storage of files across many nodes, this is done by giving each item stored a unique identifier and providing a lookup mechanism pairing nodes that hold a item of data with the items unique identifier. Any duplicates are removed across the network, this is done as two identical files will always receive the same identifier. Using this identifier a file can be looked up on the network and retrieved from those nodes that are holding the file. Using this IPFS protocol as the basis for the DFS on Catalyst means that the information that is transacted on the network does not need to be stored `on chain', meaning that there is a separation of DLT and stored data, allowing nodes to pick and choose what data they hold so any information not relevant to a particular node is not held. \\ % Why is DFS different than IPFS? Why not use IPFS directly? What is different in your implementation?

On the Catalyst network every node is required to hold a DFS module. While all nodes will not hold all of the information stored on the DFS, they will be able to query and retrieve any information on the network. Furthermore, nodes will have the ability to query the existence of a file rather than being forced to download the file in order to check its existence. Nodes are required to hold the DFS module as it is used to store all information about the network.  Some of the data secured on the DFS is the critical data that is used to sync the state of new nodes joining the network. This means that this data must be accessible at all times. Therefore, if a node wishes to check any single transaction on the network or even the overall state of the ledger itself it will have to be in sync with the network, which is done through the DFS.  \\ % What information will the nodes be forced to have? Does every node have its own garbage collection implemented? How will they query the existence of files - is it content addressed? How is it content addressed?

Integrating DFS allows the Catalyst network to remain lean as nodes on the network can choose to store only the data that they choose to hold. Furthermore, it allows the storage of many rich file types in a distributed manner, meaning that they are always accessible with no down time guaranteed (excluding the potential case of the internet being down). This is further under the assumption that at least one node on the network holds the data. This also requires some nodes on the network to hold the file, if all nodes remove or unpin a file then that data will no longer be accessible to the network and must be re-added by a node. It is likely that if no nodes on the network require the file and therefore it is removed from the network that the file was redundent form use.\\ % As long as nodes continue to pin that information, sure. Shouldn't we mention that? Also, you can't ensure the uptime of the internet as a whole, so downtime is still possible.

The Catalyst network allows peers on the network to 

In this section is a description of the IPFS platform that Catalyst uses, and then how Catalyst integrates this into its ecosystem. Furthermore how the marketplace go the buying and selling of storage will operate. 