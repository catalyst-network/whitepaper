\documentclass{article}

\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{centernot}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{titlesec}
\usepackage{scrextend}
\usepackage{titlepic}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{epstopdf}
\usepackage{pdflscape}
\usepackage[margin=1.2in]{geometry}


\title{\textbf{Catalyst Network - Technical White Paper (DRAFT - V1)}}
\date{\today}
\author{Joseph Kearney\thanks{joseph.kearney@atlascity.io} - Atlas City}


\begin{document}

\includepdf [fitpaper=true] {cover-image}

\maketitle

\abstract

Distributed Ledger Technologies (DLTs) over the last decade have grown at an extraordinary rate, with new technologies being announced, developed and released on a daily basis. A majority of these technologies have not progressed from the original concepts set out by Satoshi Nakamoto in his seminal paper \cite{nakamoto2008bitcoin} with regards to speed, security, scalability and decentralisation. There has however been a clear demonstration for the appetite and innovation for wider markets to adopt these technologies. Catalyst marries together many existing technologies as well as some novel ones in order to create a framework which give a fast, scalable and fair ledger.

\begin{center}
\vspace{50mm}
\textbf{This document is a work in progress and is subject to change.}
\end{center}

\newpage

\tableofcontents

\newpage

\section*{Introduction}

DLTs are based upon the principle of a truly distributed network, allowing nodes on the network to transact both messages and objects, whether these are tokens, messages, timestamps or even rich file types. Distributed networks have many advantages over centralised systems. They prevent any one entity from holding too much influence within a system, inhibiting the influence of an unfair or malicious actor. Further, they allow trustless transactions, meaning that no member of a network has to trust any other single member on the network, and instead they facilitate trust of the network as a whole. However, distributed networks also prevent difficult challenges: particularly how do you fairly and efficiently ensure consensus among the network is reached, how are the files stored without causing the network to become extremely bloated, and how do you compute complex business logic at least as efficiently as on a centralised network. These are all problems that have been considered in the design process of Catalyst. \\

The first problem described was ensuring a fair consensus mechanism. The network coming to agreement on what transactions on a distributed network are fair and valid and those that are not are is a key issue. Early blockchain and DLTs utilised proof of work, this originates from hashcash \cite{back2002hashcash}, which was designed as a prevention mechanism for email spam. This idea was then developed for the bitcoin blockchain, it required miners to perform a computationally hard problem in order for them to expend energy and there by resources. This dissuades mining nodes from acting maliciously, as both the transactions they have added as well as the work they have performed can be verified. If their work is poor then the block they are trying to append to the blockchain will be rejected. This technique for consensus is Byzantine fault tolerant and can not be understated as the critical factor that allowed blockchain technologies to grow. However, this has two critical flaws, firstly the amount of expended and wasted energy is extraordinary especially on large networks where the potential rewards are large \\




The technologies discussed in this paper are:

\begin{itemize}
\item The Distributed File System (DFS) - Catalyst integrates a distributed file system meaning that there is a separation between the ledger and what is stored on the ledger.
\item The Kat Virtual Machine (KVM) - Based upon the innovative Ethereum Virtual Machine (EVM), the KVM improves and extends on the EVM. It allows smart contracts to be run on Catalyst.
\item The Probabilistic Byzantine Fault Tolerant (PBFT) consensus algorithm. This new consensus algorithm allows a fair and scalable mechanism for consensus and updates to the ledger to be agreed upon.
\end{itemize}



\section{Distributed File System}

Catalyst integrates its own Distributed File System (DFS) \cite{DFS} based on the InterPlanetary File System (IPFS) protocol \cite{benet2014ipfs}. DFS, as the name suggests, is a peer to peer file system, where the files contained within are distributed across and accessible to a range of peers. This file system will have no down time nor any single point of failure, meaning that they are persistent in a way that other, centralised file storage technologies are not. \\ % Why is DFS different than IPFS? Why not use IPFS directly? What is different in your implementation?

On the Catalyst network every node holds a DFS module. While all nodes will not hold all of the information stored on the DFS, they will be able to query and retrieve any information on the network. Furthermore, nodes will have the ability to query the existence of a file rather than being forced to download the file in order to check its existence. \\ % What information will the nodes be forced to have? Does every node have its own garbage collection implemented? How will they query the existence of files - is it content addressed? How is it content addressed?

Integrating DFS allows the Catalyst network to remain lean as nodes on the network can choose to store only the data that they choose to hold. Furthermore, it allows the storage of many rich file types in a distributed manner, meaning that they are always accessible with no down time guaranteed. % As long as nodes continue to pin that information, sure. Shouldn't we mention that? Also, you can't ensure the uptime of the internet as a whole, so downtime is still possible.
In this section is a description of the IPFS platform that Catalyst uses, and then how Catalyst integrates this into its ecosystem.


\subsection{IPFS}

The InterPlanetary File System forms the basis of the Catalyst DFS. IPFS is the name for a distributed file system, a collection of protocols which specify how nodes are supposed to operate in the network, and various multilingual implementations of these protocols. As it is distributed, there is no one central entity that holds and controls the flow of information. \\

In traditional computer file systems, files are indexed, referenced, and accessed through their location: for instance, `~/Documents/file.md` would refer to a Markdown file inside a "Documents" folder inside the home directory of a user on UNIX systems. However, on a distributed system, there can be no global state that could be used for addressing content. Instead of location-addressing, IPFS and other distributed systems use content-addressing.

In a content-addressed system, every file on the system is given a unique, deterministic content identifier (CID) by hashing the content. Hashing works by deterministically using the contents of the file as the input for a standard encryption algorithm. Changing the contents of a file would change the hash of the file. This CID can then be used to index, reference, and access the file directly, regardless of where it is. Advantageously, through content addressing, a malicious entity would not be able to send a user a false file in the place of the one that they have requested. This is due to the difficulty of reversing a hashing function.\\

Once a file has a CID associated with it, a distributed hash table (DHT) can be formed. DHTs are key-value stores, where the key is the CID of the relevant file, and the value is an array containing all of the user IDs of any peer which holds the file. \\

IPFS integrates Merkle Directed Acyclic Graphs (Merkle DAGs). Merkle DAGs are abstract trees which result when multiple CIDs are used as the input for another hashing algorithm, resulting in chains where entire hierarchies of files can be referenced using a single CID. Any change to any constituent file at the bottom of the tree (the leaf nodes) percolates up to the next hash, and the next, causing the CID for the entire tree to change.


\subsection{Integrating the DFS}

Each node on the running on the catalyst network must also be running the DFS module. This allows them on the most basic level to access the current state of the ledger and the validate previous updates to the ledger thereby allowing them to sync with the current ledger state. \\

The primary difference between the native IPFS protocol and the implementation utilised on Catalyst is how the peer IDs are created.  While on the IPFS protocol the identifier for nodes is selected randomly, on Catalyst the nodes on the network will each have their own individual peer identifier\cite{BytesExtentions} made up of:

\begin{itemize}
\item IP address
\item Port number
\item Public key
\end{itemize}

% Can't you create your own ID in IPFS? Why not just mandate IPFS interoperability with your own peer names? Are you forking the rest of IPFS's functionality?



\subsection{The Marketplace}

\section{KVM}

The Catalyst network \cite{KVM}

\subsection{From the EVM}

\subsection{To the KVM}



\section{Catalyst Consensus Mechanism}

On distributed networks there is no single point of trust to determine the validity of transactions, therefore concurrency must be ensured by other methods. Typically this requires a majority of the network's participants to agree on a particular update of the ledger and the account balances / holdings held within. Blockchain technologies generally employ Proof-of-Work (PoW) and occasionally Proof-of-Stake (PoS) mechanisms in order to gain consensus across a network. However, these methods are prone to increasing centralization at scale as well as in the case of PoW high energy consumption. Other networks employ a small amount of trusted nodes that ensure the validity of transactions, however this is highly centralised and almost as fallible as the single point of failure systems that DLT endeavors to avoid. \\

Catalyst integrates a newly designed consensus mechanism, based on Probabilistic Byzantine Fault Tolerance (PBFT).  This is a collaborative rather than competitive protocol, meaning that all honest work performed by nodes on the network benefits the security of the network and that all participating nodes are rewarded equally. For each ledger cycle a random selection of worker nodes are selected, the nodes become the producers for a cycle or number of cycles. The producer nodes perform work in the form of compiling and validating transaction thereby extracting a ledger state change for that cycle. \\


The protocol is split into four distinct phases:

\begin{itemize}

\item Construction Phase - Producer nodes that have been selected create what they believe to be the correct update of the ledger. They then distribute this proposed ledger update in the form of it hash digest.
\item Campaigning Phase - Producer nodes designate and declare what they believe to be the most popular ledger state update.
\item Voting Phase -
\item Synchronisation Phase - In this phase the producers who have computed the correct ledger update can broadcast this update the rest of the network.

\end{itemize}

This section is based on the work set out in \cite{catalystresearch}, where the original research into the creation of a new consensus mechanism are laid out.

\subsection{Notation}

\subsection{Producer Node Selection}



Original work for the Catalyst consensus mechanism states that the peers are selected with relation to the PID and the hash of the previous data. As the PID can be manipulated by a user and thereby weight in their favor of selection to become a producer this is not usable. \\

Research into a RANDAO provides a viable alternative \cite{skvorc}\cite{randao}. Generation of a process by which each user creates their own random value and then by combining these random numbers across the network you gain distributed generated pseudo-random numbers. The larger the network the more random the number will be. The proposed process works as follows: \\

\begin{itemize}

\item Each node $n$ in the worker pool $N$ generates a random number $r$.
\item To this random number the hash of the previous ledger state, $D$, must be added.
\item $n$ then creates a Blake-2b hash of the combined random number $H(r + D)$.
\item Each $n$ must then send their value $r$ to the contract.
\item If they do not send their $r$ value they are not eligible to become a producer node.
\item Each $n$ in $N$ sends their $H(r + D)$ to a hard coded smart contract. This creates the global random value $R$. This is done through addition of all the random numbers.
\item The smart contract must determine:
\begin{itemize}
\item Whether $n$ did submit all correct information. i.e. $r$, and $H(r+D)$.
\item That $n$ did in fact use the $D$ value when generating the random number. This is done by taking the $r$ value submitted by the user and hashing with $D$.
\item Ensuring $n$ has paid a sufficient stake to take part in the selection process.
\item Validating that each worker $n$ has only distributed one random number to the smart contract.
\end{itemize}
\item Failure of any one of these points means the smart contract will not accept the submission of a random number from $r$ and and stake made will be lost.
\item This global random can the be used to determine the producers for the next cycle(s).
\item This is done by determining the nodes that have a $H(r + D)$ closest to the $R$ value. \\

\end{itemize}


This method is secure from manipulation as hashing algorithms are one way functions meaning that there is no provably efficient method to inverse a hashing function i.e. retrieving a message $m$ from a digest $H(m)$. If a node does not input a value into the smart contract then they are not eligible for selection for becoming a producer for that cycle(s). \\

Addition of the $D$ value is necessary as this will prevent a producer from using known random values to create a desired digest that gives them an advantage when being selected. The value for $D$ must fulfill two rules, firstly it must always be the same for all nodes in the worker pool, secondly it must change with each draw of a random number when determining the random selection of producer nodes. This prevents a user creating a hash using known input and digest combinations to gain an advantage when being selected. Furthermore if the hash of the previous ledger state is used as $D$ is ensures that a prospective producer node knows the current ledger state. If they do not then their random number will be invalid as $H(r + D_{prod}) \neq H(r + D)$. \\

As described in RANDAO, this can be further extended to implement staking. Nominal fees to contribute can be added in order to prevent DDOS. This is done in such a way that a nominal fee is added as to not dissuade users from legitimately wanting to become producers while dissuading malicious entities from attempting to perform a Sybil attack against the network in order to gain majority control over a producer pool for a cycle or multiple cycle. There is also the additional benefit of simplification of the overall consensus mechanism as it removes the need for a queuing mechanism as well as producing a verifiable method of keeping track of what nodes are registering to be workers for any given cycle. It also thereby in turn provides evidence to other nodes on the network who the producers for any given cycle are as the process will be verifiable. \\

This scheme will provide the Catalyst consensus mechanism with a verifiable, reliable and secure mechanism to generate a pool of producers. By randomly assigning which workers get to participate in the next ledger cycle, fairness is ensured. \\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Construction Phase}

The first phase of the Catalyst consensus algorithm is the Construction Phase. Within which the selected producer nodes  $P$ calculate their proposed ledger state update or their local ledger state update. This is done by aggregating and validating all transactions that have occurred during a set time period. These transactions assuming their validity are integrated into the producers local ledger state update. From which they can create a hash of the update. This hash digest represents what they believe to be the correct update and is broadcast to the other producer nodes during for that cycle. Assuming the collision free nature of hash functions, the only mechanism for multiple producer nodes to have the same local ledger state update is by both using the same set of transactions. \\

\subsubsection{Local partial ledger state update generation and broadcast}

Each producer $p$ in the set of producers $P$ follows the same protocol. The construction phase begins with producer $p$ beginning their construction phase by flushing their mempool. This mempool $T$ is made up of $n$ transactions $t$  where $n$ is the number of transactions that have been broadcast to the network and have been stored by $p$. These transaction are used to create $p$'s local ledger state update $u$.  \\

The producer at this point also creates a hash trees $d$, this is to store the the signatures that are extracted from each transaction in $T$. A salt $\sigma$ is created utilizing a pseudo-random number generator using the previous ledger state update $U-1$ as its seed. $p$ then follows the following steps:

\begin{enumerate}
\item Producer $p$ verifies that each transaction in $T$ is valid following the rules set out in \cite{transactionvalidator}. From each of the $n$ transactions in $T$ the entries $E$ that constitute the transaction $t$ are extracted to form a list $E = e_1,...,e_m$ for $m$ entries in $t$. The producer should therefore end up with $n$ lists for $E$ from $T$. each signature from the transactions are also extracted and added to $d$.

\item $p$ for each $E$ it created then creates a corresponding hash digest as:
\begin{center}
$L = \mathcal{H}[E~||~\sigma]$
\end{center}

Each pair ($E,O$) is added to a list $L$.

\item $p$ then sorts list $L$ into lexicographical order according the hash values $O$.

\item The producer $p$ then extracts the transaction fee value from each transaction in $T$ to create $v$ which is the total sum of all transaction fees.

\item The local ledger state update $u$ for producer $p$ can then be calculated. Firstly the list $L$ is concatenated (denoted by $||$) with the hashtree $d$ and a hash digest is created as:

\begin{center}
$u = \mathcal{H}(L~||~d)$
\end{center}

$u$ is then concatenated with $p$'s unique peer identifier $Id$ to create:

\begin{center}
$h = u ~||~Id$
\end{center}

\item $h$ is then broadcast to the other producer nodes on the network.
\end{enumerate}

\subsubsection{Partial ledger state update collection}


Producer $p$ also collects other producers partial ledger update values. At most they will collect $P-1$ values. Optimally every producer in $P$ will receive the same set of transactions therefore for every $p$ in $P$ will have the same partial ledger update $u$. However this is unlikely due to all transactions not being received by a small group of nodes. Equally they may not hold $G$ where $G = h_1,...,h_P$, meaning they may not receive a proposed update from all candidates.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Campaigning Phase}

The second phase of the consensus mechanism is where producer $p$ designated as candidate for what it calculates to be the most popular ledger state update. \\

\subsubsection{Local candidate generation and broadcast}

Beginning this phase producer $p$ has a set of partial ledger updates $G$ that it has received from other producer nodes. Each $h$ within $G$ contains a producers hash of the proposed update ($u$ and a peer identifier ($Id$). The most popular $u$ value can be found, this gives us $u^{maj}$ from there the subset $G_{maj}$ can be created, which is the amount of votes for the most popular update. Two thresholds must be considered first is $G_{min}$ this is the minimum amount of updates it has received from other producers in order to generate a valid candidate. The second is $G_{thresh}$ this is the threshold value for which a minimum number of votes must be in favor of $G_{maj}$ which the most popular vote found within $G$.  So in order to proceed with declaring a candidate $G > G_{min}$ and $G_{maj} > G_{thresh}$. \\

If the thresholds are met the following can take place:

\begin{enumerate}
\item $p$ creates a list $\mathcal{L}(prod)$. To this list $p$ appends the identifier of any producer that correctly sent the $u$ value that equals $u^{maj}$. If $p$'s $u$ value is also the same as $u^{maj}$ then they should append their own $Id$.
\item Producer $p$ then creates their candidate for the ledger update $c$ which is calculated as $c = u^{maj}~||~\#(\mathcal{L}(prod))~||~Id$
\item Producer $p$ will then broadcast their preferred update $c$ to the other producers.
\end{enumerate}



\subsubsection{Candidate collection}

$p$ during this phase will be collecting the $c$ values from other producers. At the end of this phase of the cycle $p$ will hold a set of $C$ candidates.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Voting Phase}

The third phase of the ledger cycle is the Voting Phase within which a producer $p$ from the $C$ candidates it has received decides on what it believes should be the global ledger state update i.e. the update that should be applied to the ledger for that cycle.


\subsubsection{Ballot generation and broadcast}

\begin{enumerate}
\item $p$ verifies that the same first hash value $u^{maj}$ is embedded in a majority of producer candidates.
\item $p$ is required at this point to have created the majority ledger update. As only with this partial update can they participate effectively. Therefore their $u == u^{maj}$

\end{enumerate}

If each producer collects the first hash value generated by every producer, any two producers would build the same list of identifiers $\mathcal{L}(prod)$. However even in highly efficient distributed network it is unlikely that all producers will retrieve all information. Therefore it must be assumed that the list of identifiers held by $p$ is incomplete, $p$ however must ensure that they hold enough information to confidently issue a vote on the update to the ledger. The identifier of a producer is appended to the hash they distribute because:

\begin{itemize}
\item To verify that $p$ is a producer node and that they in fact were one of the winners of the random draw described in \ref{insert ref}.
\item To evaluate and track the quality of the work performed by $p$.
\item To ensure that the correct producers withing $P$ are rewarded for their work maintaining the ledger. \\

\end{itemize}

New transaction entries (compensation entries) are created using the list's  $\mathcal{L}(prod)$ produced by the producers $p$. This allows reward and fees to be paid to the correct users. These new transaction entries will be appended to the final ledger state update for that cycle. Therefore a complete ledger state update should consist of:

\begin{itemize}
\item The list of transaction entries integrated into the new ledger state all $E$ lists.
\item The transaction signatures from $T$ held in $d$.
\item Compensation entries rewarding the producers from $\mathcal{L}(prod)$.

\end{itemize}

The voting process thereby must confirm the correct list of identifiers involved in producing the correct ledger state updates. The final list for  $\mathcal{L}(prod)$ is generated by merging the producers lists together. In order for a producer $p$ to have their $Id$ added to the final  $\mathcal{L}(prod)$ the must appear in at least 50\% of the  $\mathcal{L}(prod)$ lists distributed by $P$. This ensures that no producer can just append their own $Id$ to their update whether that update was correct or not and gain a reward for it.  \\

As explained below the final list $\mathcal{L}_n(prod)$ is obtained by merging the partial lists included in the producers' candidate. A producer $P_j$ could have produced a first hash value $h_{\Delta j}$ different to $h^{maj}_{\Delta j}$ yet added his identifier to $\mathcal{L}_j(prod)$ when building its candidate $c_j$ in the attempt to collect some token reward. In such scenario $Id_j$ would be an element of the list included in $c_j$ (or any other producer node controlled by $P_j$), but it wouldn't be included in any other list $\{\mathcal{L}_k(prod)\} \forall~k \in P/j$. To prevent such malicious behavior, a rule imposes that $P_j$ only appends to the final list $\mathcal{L}_n(prod)$ the identifier of a producer included in the list $\mathcal{L}_k(prod)$ of a candidate $c_k$ satisfying $h^{maj}_{\Delta k} = h^{maj}$ if and only if that identifier is included in at least $P/2$ lists $\{\mathcal{L}_{k}(prod)\}_{k=1,..,V_j}$ associated to a candidate $c_{k}$ satisfying $h^{maj}_{\Delta k} = h^{maj}$. Only a producer controlling half or more of the producer nodes would succeed in including its identifier into the final list $\mathcal{L}_n(prod)$.\\

Although this eliminates the risk of unethical behavior from the producer, this also means that there would be little incentive for a producer to broadcast its vote if its identifier was not included in $\mathcal{L}_n(prod)$. However, the probability that a producer compiles the correct final list $\mathcal{L}_n(prod)$ strongly depends on the number of votes collected. The more votes collected by a producer, the greater the probability that said producer will compile the complete final list. Although a producer may not have produced the correct partial ledger state update, participating in the voting process is, therefore, an important contribution to the overall consensus protocol and should entitle the producer nodes to some reward. To that end a producer $P_j$ can use the identifier of other producers included in their vote and create a second list $\mathcal{L}_j(vote)$ to account for their participation in the voting process. \\

$p$ follows a series of steps:

\begin{enumerate}
\item $p$ generates a list $\mathcal{L}(vote)$ it then appends the $Id$ of any other $p$ that has forwarded a $C$ candidate that satisfies $u == u^{maj}$.
\item $p$ creates the combined list for $\mathcal{L}(prod)$ of any $p$ from $P$ that appears in $P/2$ lists.
\item Then $p$ creates a list $L_{CE}$. This contains the contribution entries. This is for any user that is included in the combined $\mathcal{L}(prod)$. Each of these producers contained within this list will receive $x$ tokens. If it is considered that $X$ is the total number of tokens that are injected into the network at each cycle for the pool of $P$ producers. The amount of entries into $L_{CE}$ will be less than or equal to $P$. $R$ is to be considered the total amount of reward that is to be split among all the producers that found the correct ledger update $R = f_{prod}X + v$ where $f_prod$ is the fraction of the injected tokens that are given to users found in the combined $L_{prod}$. While $(1-f_{prod})X$ is the value that is distributed to other nodes that have performed work.
\item Producer $p$ can then create the candidate ledger state update for this cycle. This includes the reward allocations for producers according to their contributions. This ledger state update $LSU$ can be defined as:

\begin{center}
$\mathbf{LSU = L~||~d~||~L_{CE}}$
\end{center}


$p$ then computes its vote (or \textit{producer vote}):
\begin{center}
\fbox{$V = \mathcal{H}(LSU)~||~\#(\mathcal{L}(vote))~||~Id$}
\end{center}


\item $p$ then forwards their $V$ to the other producers and collects the producer votes $V$ issued by its peers.

\end{enumerate}


\subsubsection{Ballot collection}
During the voting phase, the producer $p$ collects the producer votes $V$ broadcast by its peers. At the end of the voting phase , the producer $p$ holds $U$ producer votes $V$ in its cache. This will be equal to or less than the number of producers who formed the correct update.

\subsection{Synchronisation Phase}

\subsubsection{Final ledger state update generation and broadcast}



During the synchronisation phase $p$ executes the following steps:

\begin{enumerate}
\item $p$ defines the ledger state update $\Delta L_n$ for the cycle $\mathcal{C}_n$ as:\\
 $\mathcal{H}(\Delta L_n) = max[unique(\mathcal{H}(LSU_k))~\forall~k\in\{U_j\}]$ and the associated number of votes collected: $U^{maj} = count[(\mathcal{H}(LSU_k) = \mathcal{H}(\Delta L_n))~\forall~k\in\{U_j\}]$ and verifies that $U^{maj}> U_{threshold}$.
\item $P_j$ creates a new list $\mathcal{L}_{n}(vote)$ and append to $\mathcal{L}_{n}(vote)$ the identifier of a producer included in the list $\mathcal{L}_{k}(vote)$ of a vote $v_k$ satisfying $\mathcal{H}(LSU_k) = \mathcal{H}(\Delta L_n)$ if and only if the identifier is included in at least $C_n/2$ lists $\{\mathcal{L}_{k}(vote)\}$ associated to a producer vote $v_{k}$ satisfying $\mathcal{H}(LSU_k) = \mathcal{H}(\Delta L_n)$. Note that $C_n$ can be easily computed as it corresponds to the number of producer identifiers who correctly computed the ledger state update and are therefore included in $\mathcal{L}_{n}(prod)$.

\item If $P_j$ generated the correct ledger state update $\Delta L_n$, it can write it to DFS which will return it with a content-based address $\mathcal{A}_n$.
\item A Producer $P_j$ then creates the following output quantity (or \textit{producer output}):
\begin{equation}
\fbox{$\mathbf{o_{j} = \mathcal{A}_n~||~\#(\mathcal{L}_{n}(vote))~||~Id_j}$}
\label{eq:Hj}
\end{equation}
The producer then broadcasts $o_j$ to the network.
\end{enumerate}


\subsubsection{Ledger state synchronisation across the network}

During the time period [$t_{s}, t_s + \Delta t_{cycle}$], user nodes collect $\{o_k\}_{\forall k \in P}$ producer outputs broadcast by the producers.
By extracting the identifier $Id_k$ embedded in any collected output $o_k$, a user node can easily compile a list of producer identifiers having broadcast the same second hash value $\mathcal{H}(\Delta L_n)$ (concatenated with the same list $\mathcal{L}_{n}(vote)$). Upon receiving $x > P/2$ identical addresses $\{\mathcal{A}_k = \mathcal{A}_n\}_{k \in x}$, the user nodes can read the common address content ($\Delta L_n$) from DFS. Using $\Delta L_n$ a user node can safely synchronise their local copy of the ledger and write it to their DFS if not already done. The balance of accounts stored on the ledger are updated and the producers effectively collect their rewards.\\


Worker nodes also store the list $\mathcal{L}_{n}(vote)$ embedded in each $o_k$ output. If selected to be a producer for the next cycle $\mathcal{C}_{n+1}$, a worker can use it to generate the reward allocated to the producers who correctly voted for the accurate ledger state update during the ledger cycle $\mathcal{C}_{n}$.\\


The various parameters and thresholds mentioned in this chapter and their impact on the levels of security and confidence in the successful production of a ledger state update are discussed in section~\ref{Sec:SecLsu}.



\bibliography{catalyst-technical-whitepaper}
\bibliographystyle{ieeetr}

\end{document}